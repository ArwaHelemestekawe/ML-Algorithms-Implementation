{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nR-squared: r_squar = 1 - (sse/sst):\\n computes the R-squared value, \\n which represents the proportion of the variance\\nfor y that’s explained by the independent variables\\n in the model. It’s calculated\\n as 1 minus the ratio of the sum of squared errors \\n (SSE) to the total sum of squares (SST).\\n  **** A higher R-squared value indicates a\\n    better fit of the model.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class gradient_regression:\n",
    "    def __init__(self,iterations =1000,learning_rate=0.01,type=\"batch\",batch_size=20,penality=None,alpha=.1,l1_ratio=.5):\n",
    "           self.learning_rate = learning_rate\n",
    "           self.iterations= iterations\n",
    "           self.type=type\n",
    "           self.batch_size= batch_size\n",
    "           self.alpha=alpha\n",
    "           self.penality= penality\n",
    "           self.l1_ratio=l1_ratio\n",
    "           self.weights= None\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "          n_sampels ,n_features = x.shape\n",
    "          # features = coloumns\n",
    "          # n_sampels = rows باختصار يعني بيعملها زي بنداس \n",
    "          self.weights = np.zeros(n_features+1)\n",
    "          # assume that all weights is zero in the bigining and +1 becuase there is coloumn(هنا ضافه كانه صف) of b0 we want to add\n",
    "          # include bias term b0 هيزود عمود علي عدد \n",
    "          bias = np.ones(n_sampels)\n",
    "          #عايز قصاد كل سامبل يكون ليها b0 فهنا هو عمل اراي من الوحايد \n",
    "          x = np.c_[bias,x]\n",
    "          for iteration in range (self.iterations):\n",
    "                if self.type==\"batch\":\n",
    "                      gradient = self._compute_gradient(x,y)\n",
    "                elif self.type==\"stochastic\":\n",
    "                      index = np.randome.choice(n_sampels)\n",
    "                      gradient =self._compute_gradient(x[[index]],y[[index]])\n",
    "                      # في مشكلة حصلت هنا لما جيت اضرب المصفوفة \n",
    "                      # مش فاهمة  اوي ايه المشكلة بس خلاصة اني ممكن احولها اراي فيها عنصر واحد\n",
    "                elif self.type ==\"minibatch\":\n",
    "                      indices = np.random.choice(n_sampels,self.batch_size,replace=False)\n",
    "                      '''\n",
    "                     n_samples: This is the 1-D array that you’re sampling from.\n",
    "                     In this case, it’s likely an array of indices for your data. زي فضاء العينة\n",
    "                     \n",
    "                     self.batch_size: This is the number of\n",
    "                     (unique) elements you want to sample from n_samples\n",
    "                      '''\n",
    "                      gradient = self._compute_gradient(x[indices],y[indices])\n",
    "                      '''\n",
    "                     x[indices], selecting the rows of x that correspond to the indices in your mini-batch. \n",
    "                     y[indices] is selecting the corresponding labels for the mini-batch\n",
    "                      '''\n",
    "                else:\n",
    "                      raise ValueError(\"only....etc\")\n",
    "                self.weights-=self.learning_rate*1/(2*n_sampels)*gradient\n",
    "          '''\n",
    "          The line x = np.c_[bias,x] is adding a new column of ones to the input data x, representing the bias term.\n",
    "          This does not involve the weights directly, but it prepares\n",
    "           the input data for the weighted sum computation, where each input value (including the bias) is multiplied by its corresponding weight.\n",
    "           The result x is now a matrix where each row represents a sample,\n",
    "           each column represents a feature,\n",
    "           and the first column is all ones, representing the bias term.\n",
    "            '''\n",
    "    def _compute_gradient(self,x,y):\n",
    "        gradient = -2*x.T.dot(y)+2*x.T.dot(x).dot(self.weight)\n",
    "        # penalty \n",
    "        if self.penality is not None:\n",
    "              if self.penality ==\"l1\": \n",
    "                    penality =self.alpha*np.sign(self.weights)\n",
    "              elif self.penality ==\"l2\":\n",
    "                    penality = 2*self.alpha*self.weights\n",
    "              elif self.penality==\"elastic_net\":\n",
    "                    l1_penality=self.l1_ratio*self.alpha*np.sign(self.weights)\n",
    "                    l2_penality=(1-self.l1_ratio)*self.alpha*self.weights\n",
    "                    penality=self.alpha*(l1_penality+l2_penality)\n",
    "\n",
    "\n",
    "              else:\n",
    "                    print(\"error\")\n",
    "              gradient[1:]+=penality[1:]\n",
    "              # بيشيل b0\n",
    "        return gradient\n",
    "    \n",
    "\n",
    "    def predict(self,x):\n",
    "          bias =np.ones(x.shape[0])\n",
    "          x = np.c_[bias,x]\n",
    "          return x.dot(self.weights)\n",
    "'''\n",
    " computes the dot product of the input data\n",
    "   (including the bias term) and the weights of the model.\n",
    "    this is equivalent to the equation y=Xw+b\n",
    "\n",
    " the weights (self.weights)\n",
    " must have been learned by the model beforehand \n",
    " ( gradient descent)\n",
    " for these predictions to be meaningful.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def score (self,x,y):\n",
    "      y_pred = self.predict(x)\n",
    "      sst = np.sum((y- self.y_mean)**2)\n",
    "      sse = np.sum((y-y_pred)**2)#(دي تمام)\n",
    "      r_squar = 1-(sse/sst)\n",
    "      # accuracy for training data \n",
    "      return r_squar\n",
    "\n",
    "'''\n",
    "      Total Sum of Squares (SST): sst = (y - self.y_mean)**2 computes the total sum of squares, \n",
    "      which measures the total variance in the y data. self.y_mean is the mean (average) of y.\n",
    "      The difference (y - self.y_mean) is the deviation of y from its mean,\n",
    "      and squaring these deviations \n",
    "      \n",
    "'''\n",
    "\n",
    "\n",
    "''' \n",
    "R-squared: r_squar = 1 - (sse/sst):\n",
    " computes the R-squared value, \n",
    " which represents the proportion of the variance\n",
    "for y that’s explained by the independent variables\n",
    " in the model. It’s calculated\n",
    " as 1 minus the ratio of the sum of squared errors \n",
    " (SSE) to the total sum of squares (SST).\n",
    "  **** A higher R-squared value indicates a\n",
    "    better fit of the model.\n",
    "\n",
    "'''\n",
    "\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deviation of y from its mean, (y - self.y_mean), is a ***measure of how much each data point in y differs from the average value of y***. This is important in statistics and machine learning for a few reasons:\n",
    "\n",
    "***1-Understanding Variability:*** The deviation gives us an idea of the spread or variability of the data. If the deviations are small, it means the data points are close to the mean, indicating low variability. If the deviations are large, the data points are spread out over a wider range, indicating high variability.\n",
    "\n",
    "\n",
    "***2-Normalization:*** In many machine learning algorithms, it’s important to normalize the data so that all features have the same scale. Calculating the deviation is a part of this normalization process.\n",
    "\n",
    "***3-Calculating Other Statistics:*** The deviation is used in the calculation of many other statistical measures, like variance and standard deviation, which are used in various machine learning algorithms.\n",
    "\n",
    " squaring the deviations (y - self.y_mean)**2 and summing them up gives the Total Sum of Squares (SST). ***SST is a measure of the total variance in the data***. \n",
    "In regression analysis, SST is used as a baseline against which the performance of the model is compared. Specifically, the model’s Sum of Squared Errors (SSE) is subtracted from the SST to calculate the R-squared statistic, which tells us how much of the variance in y our model is able to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The R-squared** value is a number that tells us how well our model’s predictions match the actual data. It ranges from 0 to 1 (or 0% to 100%).\n",
    "\n",
    "If the R-squared value is 1 (or 100%), it means our model’s predictions perfectly match the actual data. \n",
    "If the R-squared value is 0 (or 0%), it means our model’s predictions don’t match the actual data at all. It’s like missing the target every time.\n",
    "So, when we say a higher R-squared value indicates a better fit of the model, it means the predictions made by the model are closer to the actual data.\n",
    "\n",
    " a high R-squared doesn’t always mean the model is good. Sometimes, a model can be too complex and might just be memorizing the data, which is not good. (overfitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
