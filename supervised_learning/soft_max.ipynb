{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Soft_max_:\n",
    "    def __init__(self,epochs=10000,learning_rate=0.1):\n",
    "        self.epochs=epochs\n",
    "        self.learning_rate=learning_rate\n",
    "        self.weights=0\n",
    "        self.biases=0\n",
    "        # every class has b0 \n",
    "        \n",
    "    \n",
    "    def soft_max(self,z):\n",
    "        exp_z=np.exp(z-np.max(z,keepdims=True,axis=1))\n",
    "        return  exp_z/np.sum(exp_z,keepdims=True,axis=1)\n",
    "    '''\n",
    "     z: This is a numpy array of real numbers which you want to transform via the softmax function.\n",
    "     The shape of z is typically (N, C) where N is the number of data points, and C is the number of classes.\n",
    "\n",
    "         number of classes\n",
    "\n",
    "    s1    [1,0,0]\n",
    "\n",
    "    s2    [0,1,0]\n",
    "  \n",
    "    \n",
    "\n",
    "Outputs:\n",
    "The function returns a numpy array of the same shape as z. Each element in z is transformed into a probability in the range (0, 1),\n",
    "and the probabilities sum to 1 along the second axis (i.e., for each data point).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Mathematical Explanation:\n",
    "The softmax function transforms each element in an input array using the exponential function,\n",
    "and then normalizes these values by dividing by the sum of all these exponentials.\n",
    "This has the effect of “squashing” the input values into the range (0, 1) and making them sum to 1, so they can be interpreted as probabilities.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "    def gradient_descent(self,x,y):\n",
    "       for i in range(self.epocks):\n",
    "           scores =np.dot(x,self.weights)+self.biases\n",
    "           prob=self.soft_max(scores)\n",
    "           gradient_w=np.dot(x.T,(prob-y))/self.num_sampeles\n",
    "           gradient_bias = np.mean((prob-y),axis=0)\n",
    "           self.weights = self.weights-(self.learning_rate*gradient_w)\n",
    "           self.biases  = self.biases-(self.learning_rate*gradient_bias)\n",
    "            # updating weights trick \n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        number_of_classes= len(np.unique(y))\n",
    "        # based on the number of unique classes in y\n",
    "        self.num_sampeles,num_features=x.shape\n",
    "        self.weights=np.zeros((num_features,number_of_classes))\n",
    "        self.biases = np.zeros(number_of_classes)\n",
    "        y=y.reshape(-1,1)  \n",
    "        #  fit_transform >>> expects a 2D array as input.\n",
    "        #  -1 is used as a placeholder that means “adjust as necessary”\n",
    "        encoder =OneHotEncoder()\n",
    "        y_encoded = encoder.fit_transform(y).toarray()\n",
    "\n",
    "        self.gradient_descent(x,y_encoded)\n",
    "        # in the end we will add b0 to each class\n",
    "        '''\n",
    "        # weights >>> from gradient \n",
    "                   class_1          class_2\n",
    "\n",
    "        f1        B1,1*x1(f1)      B1_2*x1(f1)\n",
    "\n",
    "        f2        B2,1*x2(f2)      B2_2*x2(f2)\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "        '''\n",
    "        # yk  true value  number of sampels*number of classes\n",
    "            \n",
    "         number of classes\n",
    "\n",
    "    s1    [1,0,0]\n",
    "\n",
    "    s2    [0,1,0] \n",
    "\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "       cross entropy gradient >>> 1/m  sum (p^k - y(true))*x\n",
    "        x shape>>> number of sampels * number of features\n",
    "        yk shape>>> number of sampel * number of classes\n",
    "         need to transpose x to apply matrix multiplication\n",
    "\n",
    "         xT(number of features * number of sampels) *(number of sampels* number of classes)(yhat-y)\n",
    "        \n",
    "        '''\n",
    "\n",
    "    def predict_probability(self,x):\n",
    "        scores = np.dot(x,self.weights)+self.biases\n",
    "        return self.soft_max(scores)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        probability = self.predict_probability(x)\n",
    "        return np.argmax(probability,axis=1)\n",
    " # axis=0>> coloumn >> finding max in 2d  \n",
    "    def score(self, x,y):\n",
    "        predicted_label = self.predict(x)\n",
    "        return np.mean(predicted_label == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***while one-hot encoding can be performed on raw categorical data, get_dummies is specifically a pandas function that performs one-hot encoding on the specified categorical columns of a DataFrame. So, you can say that get_dummies is a way to achieve one-hot encoding in pandas.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
